# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

workflow:
  name: pytorch-distributed-sample
  resources:
    default:
      gpu: 8
      cpu: 16
      memory: 30Gi
      storage: 30Gi
      platform: {{platform}}

  groups:
  - name: training
    tasks:
    # Worker task that will communicate with the master task
    - name: worker
      image: nvcr.io/nvidia/pytorch:24.03-py3
      command: [ "bash"]
      args: ["/tmp/entry.sh"]
      files:
      - path: /tmp/entry.sh
        contents: |
         apt update
         export DEBIAN_FRONTEND=noninteractive
         apt install -y tzdata
         ln -fs /usr/share/zoneinfo/America/Los_Angeles /etc/localtime
         apt install -y netcat dnsutils
         set -x

          # Set the MASTER_PORT environment variable for Pytorch Distributed to
          # communicate with the master task, and for the worker task to know
          # that the master task has started running distributed training
          export MASTER_PORT=29500

          # Wait until the SSH server for master is up
          while ! nc -z {{host:master}} $MASTER_PORT; do
            sleep 10;
          done

          CONTROL=$(nslookup {{host:master}} | grep -oP \
            'Address: \K\d[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}')

          # Set the MASTER_ADDR environment variable for Pytorch Distributed to
          # communicate with the master task
          export MASTER_ADDR=$CONTROL

          # Run training
          mkdir -p /models
          python3 /train_dist.py --offset 8 --size 8 --world_size 16
      - path: /train_dist.py
        localpath: scripts/train_dist.py
    # Master task for Pytorch distributed
    - name: master
      image: nvcr.io/nvidia/pytorch:24.03-py3
      lead: true
      command: [ "/bin/bash" ]
      args: ["/tmp/entry1.sh"]
      files:
        - path: /tmp/entry1.sh
          contents: |
            # Install packages
            apt update
            export DEBIAN_FRONTEND=noninteractive
            apt install -y tzdata
            ln -fs /usr/share/zoneinfo/America/Los_Angeles /etc/localtime
            apt install -y netcat dnsutils
            set -x

            # Set parameters through environment variables to tell Pytorch distributed
            # that this is the master task
            export MASTER_ADDR=localhost
            export MASTER_PORT=29500
            mkdir -p /models
            python3 /train_dist.py --offset 0 --size 8 --world_size 16

            MODEL_STORE_DIR="{{output}}/model/"
            mkdir -p $MODEL_STORE_DIR
            mv /models/ $MODEL_STORE_DIR
        - path: /train_dist.py
          localpath: scripts/train_dist.py
      volumeMounts: ['/dev/shm']
      outputs:
        - dataset:
            name: {{ dataset }}
            path: model

default-values:
  platform: ovx-a40
  dataset: model-sample
